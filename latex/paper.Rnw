\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[]{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[]{color}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{xspace}
\usepackage{calc}
\usepackage[noae]{Sweave}
\usepackage{authblk}
\usepackage{algorithm}
\usepackage{algpseudocode}
%
% New commands
%
\newcommand{\ie}{\emph{i.e.}\@\xspace}
\newcommand{\eg}{\emph{e.g.}\@\xspace}

\SweaveOpts{prefix.string=paper, keep.source=TRUE}

%%% RSS specific items

\title{Sequential rank agreement methods for comparison of lists}
\author{Claus Thorn Ekstrøm}
\author{Thomas Alexander Gerds}
\author{Andreas Kryger Jensen}
\author{Kasper Brink-Jensen}
\affil{Biostatistics, University of Copenhagen}
\renewcommand{\topfraction}{.85}
\renewcommand{\bottomfraction}{.7}
\renewcommand{\textfraction}{.15}
\renewcommand{\floatpagefraction}{.66}
\renewcommand{\dbltopfraction}{.66}
\renewcommand{\dblfloatpagefraction}{.66}
\setcounter{topnumber}{9}
\setcounter{bottomnumber}{9}
\setcounter{totalnumber}{20}
\setcounter{dbltopnumber}{9}

\begin{document}

% Use square brackets for numbering affiliations

\maketitle

\begin{abstract}
  The comparison of alternative rankings of a set of items is a
  general and prominent task in applied statistics. Predictor
  variables are ranked according to magnitude of association with an
  outcome, prediction models rank subjects according to the
  personalized risk of an event, and genetic studies rank genes
  according to their difference in gene expression levels. This
  article constructs measures of the agreement of two or more ordered
  lists. We use the standard deviation of the ranks to define a
  measure of agreement that both provides an intuitive interpretation
  and can be applied to any number of lists even if some or all are
  incomplete or censored. The main idea is then a permutation based
  and graphically approach to assess the sequential changes of
  agreement as a function of the depth of the lists. This may help to
  indentify change points in the agreement of the lists. The
  usefulness of these tools are illustrated using gene rankings, and
  using data from two Danish ovarian cancer studies on where we assess
  the within and between agreement of different statistical classification
  methods.
\end{abstract}

Key words: sequential rank agreement, order statistic, gene rankings,
methods comparison, variable selection, permutation

\section{Introduction}

A common task of scientific research is to rank items obtained by some
form of data analysis. Ranked lists occur naturally in applications of
statistics. Regression methods rank predictor variables according to
magnitude of their association with an outcome, prediction models rank
subjects according to their risk of an event, and genetic studies rank
genes according to their difference in gene expression levels across
samples.

When several rankings of the same items are available, a common
research question is to what extent do they agree. Once agreement is
established a further question is regarding the depths of agreement
starting from the top of the ranked lists. A typical situation arises
in high-dimensional genomics studies when several analysis methods are
applied to rank a list of genes according to their association with a
phenotype, treatment effect or other outcome. A measure of agreement
of gene rankings obtained by different methods could often help to
identify which genes are worth to pursue in further experiments.

Several approaches exist to measure the ``distance'' between two
ranked lists. Of these, Kendall's $\tau$ \citep{kendall1948} and
Spearman’s footrule $\rho$ \citep{Spearman1910} are among the most
well-known. But these measures do not distinguish between agreement in
the top versus towards the bottom of the lists. \citet{Shieh1998}
proposed a weighted version of Kendall's $\tau$ where each pair of
rankings can be assigned different weights, and \citet{Yilmaz2008}
proposed the $\tau_{ap}$ which places higher emphasis on the top of
the lists.
%
Spearman’s footrule uses the ranks of the variables for calculation of
the distance and the use of ranks are also employed in the $M$ measure
of \citet{Bar-Ilan2006} where the reciprocal rank differences are used
to calculate the agreement measure.

More recent approaches consider the intersection of lists as the basis
for a similarity measure. However, simple intersection also places
equal weights on all depths of the list. Thus, \citet{Fagin2003} and
\citet{Webber2010} proposed modified intersections which put more
emphasis on the top of the lists. One proposal is denoted the average
overlap (AO) and used for comparison later on. Specifically,
\citet{Webber2010} define their rank-biased overlap (RBO) by weighting
with a converging series to ensure that the top is weighted higher
than the potentially non-informative bottom of the lists.

It is possible to use the existing methods to calculate agreement of
lists until a given depth, i.e., limited to the $d$ items of each
list. However, the interpretation may not be straightforward,
especially in the case of more than two lists.

In this article we introduce sequential rank agreement for measuring
agreement among ranked lists. We provide graphical tools to analyse
the depth of agreement of a set of ranked lists. The general idea is
to define agreement based on the sequence of ranks from the first $d$
elements in each list. As agreement metric we adapt the limits of
agreement known from agreement between quantitative variables
\citep{Carstensen2010}. Our proposed approach allows to compare more
than two lists simultaneously, provides a dynamic measure of agreement
as a function of the depth in the lists, places high weight on the top
of the list, accommodates censored/incomplete lists of varying
lengths, and has a natural interpretation that directly relates to the
ranks. The graphical illustration of sequential rank agreement
potentially allows to infer a changepoint, i.e., a list depth where
there is a change in the agreement of the lists.

In settings where rankings are obtained from analysis of correlated
variables it is then possible to compare the actual results to the
expected agreement in non-informative data obtained by
randomization. In this sense our approach is a combination and
generalization of some of the ideas of \citet{Carterette2009} and
\citet{Boulesteix2009}. The former compares two rankings based on the
distance between them as measured by a multivariate Gaussian
distribution and the latter presents an overview of approaches for
aggregation of ranked lists including bootstrap and leave-one-out
jackknife approaches.

The manuscript is organized as follows: In the next section we define
sequential rank agreement for multiple ranked lists and discuss how to
handle incomplete/censored lists. In section 3 we present and discuss
approaches to evaluate the sequential rank agreement obtained from an
experiment and to identify the maximal depth until which the ranked
lists agree to certain extent. Finally we present three different
applications of the proposed sequential rank agreement approach to
real data before we discuss the findings along with possible
extensions.

<<echo=FALSE>>=
#note: always pass alpha on the 0-255 scale
makeTransparent<-function(someColor, alpha=100)
{
  newColor<-col2rgb(someColor)
  apply(newColor, 2, function(curcoldata){rgb(red=curcoldata[1], green=curcoldata[2],
    blue=curcoldata[3],alpha=alpha, maxColorValue=255)})
}
@ %

\section{Methods}

Consider a set of $P$ different items $X=\{X_1,\dots,X_P\}$. An
ordered list is a permutation function, $R: \{X_1,\dots,X_P\}\to
\{1,\dots,P\}$, such that $R(X_p)$ is the rank of item $X_p$ in the
list. The inverse mapping $\pi=R^{-1}$ assigns to rank
$r\in\{1,\dots,P\}$ the item $\pi(r)$ found at that rank. The methods
described below work for a set of $L$ lists $R_1,\dots,R_L$,
$L\geq2$. We denote $\pi_l=R_l^{-1}$ for the corresponding inverse
mappings. Panels (a) and (b) of Table~\ref{tab:example} show a
schematic example of these mappings. Thus if $\pi_l(1)=X_{34}$ then
item $X_{34}$ is ranked first in list $l$, and similarly
$R_l(X_{34})=1$.

\begin{table}[tb]
  \caption{Example set of ranked lists. (a) shows the ranked list of items for each of three lists, (b) presents the ranks obtained by each item in each of the three lists and (c) shows the cumulative set of items up to a given depth in the three lists.}
\begin{center}
  \begin{subtable}{4cm}%
    \caption{}
      \begin{tabular}{cccc}
        \hline\hline % & \multicolumn{3}{c}{List} \\
        Rank & $\pi_1$ & $\pi_2$ & $\pi_3$ \\ \hline
        1 & A & A & B \\
        2 & B & C & A \\
        3 & C & D & E \\
        4 & D & B & C \\
        5 & E & E & D \\ \hline
    \end{tabular}
  \end{subtable}
% \hfill
\hspace{1em}
%%%
  \begin{subtable}{4cm}%
    \caption{}
    \begin{tabular}{cccc} \hline\hline
  % & \multicolumn{3}{c}{List} \\
    Item & $R_1$ & $R_2$ & $R_3$ \\ \hline
    A & 1 & 1 & 2 \\
    B & 2 & 4 & 1 \\
    C & 3 & 2 & 4 \\
    D & 4 & 3 & 5 \\
    E & 5 & 5 & 3 \\ \hline
  \end{tabular}
\end{subtable}
% \hfill
\hspace{1em}
%%%
\begin{subtable}{4cm}%
  \caption{}
\begin{tabular}{cc} \hline\hline
% Depth   &     \\
Depth  &  $S_d$ \\ \hline
1 & $\{$A, B$\}$\\
2 & $\{$A, B, C$\}$ \\
3 & $\{$A, B, C, D, E$\}$ \\
4 & $\{$A, B, C, D, E$\}$ \\
5 & $\{$A, B, C, D, E$\}$ \\ \hline
\end{tabular}
\end{subtable}
\end{center}
\label{tab:example}
\end{table}

The agreement of the lists regarding the rank given to an item $X_p$ can be
measured by
\begin{equation}
  A(X_p) = f(R_1(X_p), \ldots, R_L(X_p)),
\end{equation}
for a distance function $f$. Throughout this paper we will use the
sample standard error as our function $f$ and hence use
$$A(X_p) = \sqrt{\frac{\sum_{i=1}^L (R_i(X_p) - \bar{R}(X_p))^2}{L-1}},
$$
but other choices could be made (see the discussion). The sample
standard error has an interpretation as the average distance of the
individual rankings of the lists from the average ranking.

We now describe what is exemplified in Panel (c) of Table
\ref{tab:example} and how it can be used to define \emph{sequential
  rank agreement}. For an integer $1\le d\le P$ we define the unique
set of items found by merging the first $d$ elements of each of the
$L$ lists, i.e., the set of items ranked less than or equal to $d$ in
any of the lists:
\begin{equation}
S_d = \{\pi_l(r) ; r \leq d, l = 1, \ldots, L \}.
\end{equation}
The \emph{sequential rank agreement} is the pooled standard deviation
of the items found in the set $S_d$:
\begin{equation}
\textrm{SRA}(d)= \sqrt{\frac{\sum_{\{p \in
      S_d\}}(L-1)A(X_p)^2}{(L-1)|S_d|}}. \label{def:sra}
\end{equation}
Values of \textrm{SRA} close to zero suggests that the lists agree
while larger values suggests disagreement. If the ranked lists are
identical then the value of SRA will be zero for all depths $d$. The
sequential rank agreement can be interpreted as the average distance
of the individual rankings of the lists from the average ranking for
each of the items we have seen until depth $d$. Note that the terms
that are part of \eqref{def:sra} are not independent since each list
contains the ranks from 1 to $P$ exactly once. Hence we simply use the
pooled standard deviation as a measure of the average distance between
rankings.

\subsection{Agreement among fully observed lists}
\label{sec:amfol}

The simplest case occurs when all $L$ lists are fully observed, \ie,
we have observed the rank of all $P$ items for all $L$ lists. Fully
observed ranked lists are common and arise, for example, when
different statistical analysis methods are applied to a single dataset
to produce lists of predictors ranked according to their importance,
or if the same analysis method is applied to data from different
populations.

With fully observed lists we can plot the sequential rank agreement
\eqref{def:sra} as a function of depth $d$. An example is seen in top
panels of Figure~\ref{fig:example1} where four analysis methods were
used to rank 3051 gene expression values measured on 38 tumor mRNA
samples in order to classify between acute lymphoblastic leukemia
(ALL) and acute myeloid leukemia (AML)
\citep{Golub1999}. Preprocessing of the gene expression data was done
as described in \citet{Dudoit2002} and the four different analysis
approaces were: marginal two-sample $t$ tests, marginal logistic
regression analyses, logistic regression eleastic net
\citep{friedman2010regularization}, and marginal maximum information
content correlations (MIC) \citep{Reshef2011}. For the first two
methods, the genes were ranked according to $p$ value, for logistic
regression the genes were ordered by size of the corresponding
coefficients (after standardization), and MIC was ordered by
correlation which resulted in the rankings seen in Table~\ref{tab1}.
The sequential rank agreement shown in Figure~\ref{fig:example1} shows
for each depth $d$ how many ranks apart do we on average expect the
genes found in the top $d$ part of the lists. It can be seen that
agreement is higher in the top of the lists (smaller values on the $y$
axis) correspond to \emph{better} than in the tails of the lists.

<<example1, echo=FALSE, eval=FALSE>>=
library(SuperRanker)
library(glmnet)
library(minerva)
library(changepoint)
library(randomForestSRC)
# Read data
library(multtest)
data(golub)

y <- golub.cl
x <- golub

topk <- 20

producelists <- function(x, y) {
    nitems <- nrow(x)
    index <- seq(1, nrow(golub))
    ## d <- data.frame(y, x)

    ## Marginale t-tests
    mt.p <- sapply(index, function(i) { t.test(x[i,] ~ y)$p.value } )
    list1 <- order(mt.p)

    ## Marginale logreg-tests
    mlogreg.p <- sapply(index, function(i) { drop1(glm(y ~ x[i,], family=binomial), test="Chisq")[2,5] } )
    list2 <- order(mlogreg.p)

    ## Elastic net
    X <- scale(t(x))
    enet <- glmnet(X, y, family="binomial", alpha=.8)
    nyres <- cv.glmnet(X, y, family="binomial", alpha=.8)
    coefficients <- coef(enet, s=nyres$lambda.1se)[-1]
    nonzeros <- sum(coefficients!=0)
    list3 <- order(abs(coefficients), decreasing=TRUE)
    if (nonzeros<nitems) {
        list3[(nonzeros+1):nitems] <- NA
    }
    ## MIC
    MIC <- sapply(index, function(i) { mine(x[i,], y)$MIC})
    list4 <- order(MIC, decreasing=TRUE)


                                        # Random Forest
###    dd <- data.frame(y=factor(y), t(x))
                                        # dd <- data.frame(y=y, t(x))
###    f1 <- rfsrc(y ~ ., data=dd, ntree=100)
###    variables <- abs(f1$importance[,1])
###    num.undecided <- sum(variables==0)
###    list5 <- order(variables, decreasing=TRUE)
###    list5[(length(variables)-num.undecided):length(variables)] <- 0

    cbind(list1,list2,list3,list4)
}

## Original run
B <- 20
inputmatrix <- producelists(x, y)
colnames(inputmatrix) <- c("T", "LogReg", "ElasticNet", "MIC", "RF")[1:ncol(inputmatrix)]
res <- sra(inputmatrix, B=B)

## Now make the same run but where we only have top 50 lists
inputmatrix2 <- inputmatrix
inputmatrix2[(topk+1):nrow(inputmatrix),] <- NA
res2 <- sra(inputmatrix2, B=B)

@


<<fig1a, fig=TRUE,echo=FALSE,include=FALSE,cache=TRUE,eval=FALSE>>=
ysize <- 1000
nreference <- 20
## Plot the line
plot(res[1:ysize], col="black", lwd=3, type="l", ylim=c(0,1300), ylab="Sequential rank agreement", xlab="Depth")

## Compute random list backgrounds
null <- random_list_sra(inputmatrix, B=B, n=nreference)

bcolor <- makeTransparent("red", alpha=80)
bcolor2 <- makeTransparent("blue", alpha=80)

## Plot the random lists
www <- smooth_sra(null)
polygon(c(1:length(www$lower), rev(1:length(www$lower))),
        c(www$lower, rev(www$upper)),
        col=bcolor2, border=NA)

## Load the data for the methods
makereference <- function(x, y, n=100, B=20) {
    sapply(1:n, function(i) { sra(producelists(x, sample(y)), B=B) }  )
}
## Uncomment the two lines below to redu the analysis
###null2 <- makereference(x, y, n=100, B=B)
###save(null2, file="R/fig1null2.rda")
load("R/fig1null2.rda")

## Plot the reference sras
www2 <- smooth_sra(null2)
polygon(c(1:length(www2$lower), rev(1:length(www2$lower))),
        c(www2$lower, rev(www2$upper)),
        col=bcolor, border=NA)

@ %


<<fig1b, fig=TRUE,echo=FALSE,include=FALSE,cache=TRUE, eval=FALSE>>=
## Same as above but zoomed in
ysize <- 50
plot(res[1:ysize], lwd=3, col="black", type="l", ylim=c(0,1300), ylab="Sequential rank agreement", xlab="Depth")

## Plot the random lists
polygon(c(1:length(www$lower), rev(1:length(www$lower))),
        c(www$lower, rev(www$upper)),
        col=bcolor2, border=NA)

## Plot the reference sras
www2 <- smooth_sra(null2)
polygon(c(1:length(www2$lower), rev(1:length(www2$lower))),
        c(www2$lower, rev(www2$upper)),
        col=bcolor, border=NA)


@ %




<<fig2a, fig=TRUE,echo=FALSE,include=FALSE,cache=TRUE, eval=FALSE>>=
ysize <- 1000
nreference <- 20
## Plot the line
plot(res2[1:ysize], col="black", lwd=3, type="l", ylim=c(0,1300), ylab="Sequential rank agreement", xlab="Depth")

## Compute random list backgrounds
null <- random_list_sra(inputmatrix2, B=B, n=nreference)

## Plot the random lists
www <- smooth_sra(null)
polygon(c(1:length(www$lower), rev(1:length(www$lower))),
        c(www$lower, rev(www$upper)),
        col=bcolor2, border=NA)

## Load the data for the methods
makereference <- function(x, y, n=100, B=20) {
    sapply(1:n, function(i) { mymat <- producelists(x, sample(y)) ; mymat[(topk+1):nrow(mymat),] <- NA ; sra(mymat, B=B) }  )
}
## Uncomment the two lines below to redu the analysis
##null2 <- makereference(x, y, n=100, B=B)
##save(null2, file="R/fig1null2c.rda")
load("R/fig1null2c.rda")

## Plot the reference sras
www2 <- smooth_sra(null2)
polygon(c(1:length(www2$lower), rev(1:length(www2$lower))),
        c(www2$lower, rev(www2$upper)),
        col=bcolor, border=NA)

@ %


<<fig2b, fig=TRUE,echo=FALSE,include=FALSE,cache=TRUE, eval=FALSE>>=
## Same as above but zoomed in
ysize <- 50
plot(res2[1:ysize], lwd=3, col="black", type="l", ylim=c(0,1300), ylab="Sequential rank agreement", xlab="Depth")

## Plot the random lists
polygon(c(1:length(www$lower), rev(1:length(www$lower))),
        c(www$lower, rev(www$upper)),
        col=bcolor2, border=NA)

## Plot the reference sras
www2 <- smooth_sra(null2)
polygon(c(1:length(www2$lower), rev(1:length(www2$lower))),
        c(www2$lower, rev(www2$upper)),
        col=bcolor, border=NA)


@ %


<<label=tab1,echo=FALSE,results=tex, eval=FALSE>>=
library(xtable)
xxx <- as.data.frame(cbind(1:10, inputmatrix[1:10,]))
colnames(xxx) <- c("Ranking", colnames(xxx)[2:5])
xxx2 <- xtable(xxx, caption = "Top 10 list of ranked results from the Golub data. Numbers indicate the predictor/gene for the given ranking and method")
digits(xxx2) <- 0
print(xxx2, table.placement = "tb", caption.placement = "top", include.rownames=FALSE)
#print(xtable(xxx,  label = "tab:one",

@


\begin{figure}[tb]
   \begin{center}
 \includegraphics[width=.45\textwidth]{paper-fig1a}
 \includegraphics[width=.45\textwidth]{paper-fig2a}
 \includegraphics[width=.45\textwidth]{paper-fig1b}
 \includegraphics[width=.45\textwidth]{paper-fig2b}
 \end{center}
 \caption{Left panels: Sequential rank agreement for 4 different
   analysis methods applied to the 3051 genes in the Golub data. Right
   panels: Corresponding sequential rank agreement for the same data
   but where only the top 20 ranked items are available. The blue and
   red areas correspond to the independent and randomized reference
   hypothesis areas, respectively. The bottom plots are identical to
   the corresponding top plots but have been zoomed in on the first
   part of the $x$ axis.}
 \label{fig:example1}
 \end{figure}



If the lists would agree on all items then we would expect the
sequential rank agreement curve to be flat. And if there are changes
in the levels of rank agreement then this would suggest that there are
sets of items for which the lists are in high agreement and sets of
other items where the lists rank the items differently. In gene
association studies we generally expect the agreement among the lists
to be better towards the top of the lists and worse towards the end of
the lists. In this case the sequential rank agreement curve will start
at a low level and then increase until it levels off as in
Figure~\ref{fig:example1}.

\subsection{Analysis of incomplete/censored lists}

Incomplete lists are also a common occurrence. They arises for example
in case of missing data (items) or when some methods only rank a
subset of the items. For example, penalized regression based on the
Lasso provides a sparse set of predictors that have non-zero
coefficients. There is no obvious ordering of the set of predictors
whose coefficient has been shrunk to zero. Incomplete lists also occur
when the analyst decides to censor all lists at depth $d$ such that
the measurement of agreement is restricted to the first $d$ elements
of each list.

Sequential rank agreement can be generalized to incomplete/censored
lists in the following way. Let $\Lambda_P$ be a set of $P$ items and
$\Lambda_l\subset \Lambda_P$ the subset of $d_l$ items that occur in
list $l$.  The case where all lists are censored at depth $d$
corresponds to $d_1=\cdots=d_L=d$. For incomplete/censored lists the
rank function becomes
\begin{equation}
  \tilde R_l(X_p) = \left\{\begin{array}{cl} \{\pi_l^{-1}(p)\} & \text{ for } p\in \Lambda_l \\
      \{d_l+1,\dots,P\} & \text{ for } p \not\in \Lambda_l\end{array}\right.
\end{equation}
where we only know that the rank for the unobserved items in list $l$
must be larger than the largest rank observed in that list.

The agreement, $A(X_p)$, cannot be computed directly for all
predictors in the presence of censored lists because the exact rank
for some items will be unknown. Also, the rankings within a single
list are not independent since each rank must appear exactly once in
each list. Thus, we cannot simply assign the same number (e.g., the
mean of the unassigned ranks) to the censored items since that would
% @Claus: less variation of the ranks? or of the agreement?
result in less variation of the agreement, and it would artificially
introduce a (downward) bias of agreement for items that are censored
in multiple lists.

Instead we randomize the ranks $\{d_{l}+1,\dots,P\}$ to the items that
do not occur in list $\Lambda_l$. By randomizing the missing items of
each list once we attain one realization of the $L$ rankings of the
set $\Lambda_P$. By randomizing a large number of times we can compute
\eqref{def:sra} for each realization, and then compute the sequential
rank agreement as the pointwise (for each depth) average of the rank
agreements. The algorithm is described in detail in
Algorithm~\ref{sra-algorithm}.

\begin{algorithm}
\caption{Sequential rank agreement algorithm for censored lists}
\label{sra-algorithm}
\begin{algorithmic}[1]
  \Procedure{Censored rank agreement}{} \State Let $B$ be the number
  of permutations to use \For{each $b \in B$} \For{each censored list
    $l \in L$}
  \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent-1.7cm}{Permute
    the unassigned ranks, $\{d_l+1, \ldots, P\}$, and assign them
    randomly to the items \emph{not} found in the list, i.e.,
    $\Lambda^\complement=\Lambda_P\setminus\Lambda_l$. Combine the
    result with $\Lambda_l$ to fill out the list.}
\EndFor
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent-1cm}{Let sra($b$) be the sequential rank agreement computed from the
filled out lists.}
\EndFor
\State Return element-wise averages across all $B$ permutations of sra($b$).
\EndProcedure
\end{algorithmic}
\end{algorithm}

The proposed approach is based on two assumptions: 1) that the most
interesting items are found in the top of the lists, and 2) that the
censored rankings provide so little information that it is reasonable
to assume that they can be represented by a random order. The first
assumption is justifiable because we have already accepted that it is
reasonable to rank the items in the first place. The second assumption
is fair in the light of the first assumption provided that we have a
``sufficiently large'' part of the top of the lists available.

When the two assumptions are satisfied then it is clear that the
interesting part of the sequential rank agreement curves is restricted
to the depths where the percentage of censored items is
low. Generally, without additional prior knowledge about the
distribution of the censored items it seems reasonable to restrict the
attention of the sequential rank agreement to depths smaller than
$\max(d_1, \ldots, d_L)$.

Like for fully observed lists we generally expect the sequential rank
agreement to start low and then increase unless the lists are
completely unrelated (in which case the sequential rank agreement will
be constant at a high level) or if the lists mostly agree on the
ranking (in which case the sequential rank agreement will also be
constant but at a low level). For censored lists we also expect a
change point around the depth where the lists are censored. This is an
artefact stemming from the fact that we assume that the remainder of
the lists can be replaced by a simple permutation of the missing
items.

\section{No information agreement}

We now consider the special case where the rankings are obtained by
applying different methods to the same data set which contains one
outcome variable and a list of candidate predictors. To interpret the
sequential rank agreement values in this situation we propose two
different benchmark values corresponding to two different
hypotheses. We wish to determine if we observe better agreement than
what would be expected if there were no relevant information available
in the data.

The first reference hypothesis is
\begin{eqnarray*}
H_0  & : &  \text{The list rankings correspond to complete randomly}\\
       &  & \text{permuted lists}.
\end{eqnarray*}
This does not only assume that there is no information data on which
in the rankings are based but also that the methods are completely
independent.

Alternatively, we can remove the restriction on the independence among
the methods by only requiring that there is no information in the
ranking but that the rankings are all based on applying the
method/approaches to the same data
% @Claus: maybe H_1 is not a good name as people would expect H_1 to be the complement of H_0
\begin{eqnarray*}
\tilde H_0 & :&  \text{The list rankings are based on data containing}\\
& &   \text{no association to the outcome.}
\end{eqnarray*}
$H_0$ is a quite unrealistic null hypothesis but we can easily obtain
realizations from that null hypothesis simply by permuting the items
within each list and then computing the sequential rank agreement for
the permuted lists. In the uncensored case each experiment contains
$L$ lists of random permutations of the numbers $1,\dots,P$. For the
censored/incomplete case we first permute the numbers $1,\dots,P$ and
then censor list $l$ at $d_l$. The sequential rank agreement curve
from the the original lists can then be compared to the pointwise rank
agreements obtained under $H_0$.

To obtain the distribution under $\tilde H_0$ the idea is to repeat
many times the ranking procedures in uncorrelated data.  Thus, we
first permute the outcome variable of the data set on which the
rankings are based. This removes any association between the predictor
variables and the outcome. Then we apply the methods to the permuted
data set to generate $L$ new rankings and subsequently compute the
sequential rank agreement. Note that we only permute the outcomes and
preserve the structure of the candidate predictors. The randomization
approach requires that we have the original data available and as such
it may not be possible to evaluate $\tilde H_0$ in all situations.

If the sequential rank agreement for the original data lies
substantially below the distribution of the sequential rank agreements
obtained under either $H_0$ or $\tilde H_0$ then this suggests that
the original ranked lists agree \emph{more} than what we would expect
in data with no information.

For the analysis of the Golub data described in Section \ref{sec:amfol},
Figure~\ref{fig:example1} shows the empirical distributions of
sequential rank agreement under $H_0$ and $\tilde H_0$ each based on
$400$ permutations. Not surprisingly, the sequential rank agreement
under $\tilde H_0$ is lower than the sequential rank agreement under
$H_0$ because the four methods used to rank the data ($t$ test,
logistic regression, elastic net, and MIC) generally tend to identify
(and rank high) similar predictors even if there are only spurious
associations. The two bottom panels in
% @Claus jeg forstaar ikke den den naste lange saetning. mener du full lists i stedet for full dataset?
Figure~\ref{fig:example1} also indicate that the observed sequential
rank agreement is better than what would be expected for the full
dataset but that the censored data (the bottom-right plot) suggests
that there may be at most 1 or 2 ranked items that are towards the top
of the lists that yield a result better than what would be expected.

It is important to stress that neither $H_0$ nor $\tilde H_0$ are
related to questions regarding the association between the outcome and
the predictors in the data set. Both hypotheses are purely considering
how the rankings agree in a situation where there is no relevant
information available in the data used for creating the rankings.

\section{Comparison to average overlap}

The average overlap is widely used in comparisons of (two) ranked
lists \citep{Fagin2003,Webber2010} and the idea behind the average
overlap closely resembles the sequential rank agreement. In this
section we present an extention of the average overlap to more that
two lists and compare this to sequential rank agreement.

The overlap among $L$ lists observed until depth $d$ is defined as the
proportion of items that are found in all $L$ lists compared to the
possible number of items found in all lists observed until depth $d$:
\begin{equation}
  O(d) = \frac{| \cap_{l=1}^L \{\pi_l(r); r\leq d \} |}{d}. \label{def:overlap}
\end{equation}
Here $|A|$ is the cardinality of the set $A$.
Originally the overlap was only considered for pairwise comparisons
but formula \eqref{def:overlap} can be applied when $L>2$.
% @Claus: har du en reference hvor OA bliver brugt paa to lister?
The average overlap at depth $d$ is defined as the average of the
overlaps until depth $d$,
\begin{equation}
AO(d) = \sum_{i=1}^d \frac{O(d)}{d}.
\end{equation}
This definition directly ensures that further emphasis is put on items
in the top of the lists since their item specific overlap contributes
to the calculation of average overlap at higher depths.

There are three main drawbacks of the average overlap method: First it
is highly sensitive to single items in the top of one the lists.
% @Claus: jeg forstaar ikke den naeste saetning.
The average overlap profile may change dramatically depending on the
occurrence of similar items in the lists. Secondly, when the number of
lists, $L$, is high then it might be difficult to obtain a non-zero
overlap (and hence a non-zero average overlap) at the beginning of the
lists because the overlap will be zero until an item is present in all
$L$ lists. Finally, the average overlap has an interpretation in terms
of moving averages of percentages (of $L$ sets) which is somewhat less
intuitive than our proposed average rank distance.

In the following we compare the average overlap and the sequential
rank agreement using the analysis of the Golub data \citep{Golub1999}
of Section \ref{sec:amfol}. We consider a typical application of the
average overlap and thus restrict attention to only two gene rankings
obtained by the $t$ tests and logistic regression. These two marginal
modelling approaches should result in very similar gene rankings. The
first two columns of Table~\ref{tab1} show the actual top-10 ranking
of the predictors.

<<fig-ao1, echo=FALSE, fig=TRUE,include=FALSE, eval=FALSE>>=
myres <- average_overlap(inputmatrix[,1:2])
myres2 <- average_overlap(inputmatrix[-c(1,2),1:2])
xvals <- 1:500
plot(xvals, myres[xvals], xlim=c(1, 150), ylim=c(0,1), type="l", lwd=2, ylab="Average overlap", xlab="List depth")
lines(xvals[-c(1,2)], myres2[1:498], col="red", lwd=2)
@

<<fig-ao2, echo=FALSE, fig=TRUE,include=FALSE, eval=FALSE>>=
mmyres <- sra(inputmatrix[,1:2])
mmyres2 <- sra(inputmatrix[-c(1,2),1:2])
xvals <- 1:500
plot(xvals, mmyres[xvals], xlim=c(1, 150), ylim=c(0, 110), type="l", lwd=2, ylab="Sequential rank agreement", xlab="List depth")
lines(xvals[-c(1,2)], mmyres2[1:498], col="red", lwd=2)
@

%% \begin{figure}[tb]
%% \begin{center}
%% \includegraphics[width=.45\textwidth]{paper-fig-ao1}
%% \includegraphics[width=.45\textwidth]{paper-fig-ao2}
%% \end{center}
 %% \caption{Average overlap (left figure) and sequential rank agreement
   %% (right figure) for comparing two different analysis methods
   %% (marginal $t$ test and logistic regression)
   %% applied to the Golub data. The black lines are based on the full
   %% set of predictors while the red lines have the two highest
   %% associated predictors removed
   %% from the data before computing the average overlap and sequential
   %% rank agreement.}
 %% \label{fig:case1}
%% \end{figure}

The black lines Figure~\ref{fig:case1} show the average overlap and
sequential rank agreement for these data. It is clear from both plots
that there is perfect agreement towards the top of the ranked lists:
the average overlap is 1 and the sequential rank agreement is zero.
However, if we remove those two items from the data (i.e.,
gene/predictor 2124 and 896) and redo the analyses then we get
substantially different curves for the average overlap but not for
sequential rank agreement (red lines in Figure~\ref{fig:case1}). For
the sequential rank agreement we get roughly the same estimate of
agreement from rank 3 as we did from the full dataset. This indicates
that sequential rank is more robust against small pertubations of the
data. For the average overlap the curves completely change when these
two items are removed and which would lead to a different conclusion
about the agreement among the items from rank 3 downwards.

\section{Applications}

We now consider an application of the sequential rank agreement to two
data sets consisting of MALDI-TOF (Matrix-Assisted Laser
Desorption/Ionization Time Of Flight) mass spectra obtained from blood
samples from patients with either benign or malignant ovarian
tumors. The data sets are subsamples of the Danish MALOVA and DACOVA
study populations.

The MALOVA study is a multidisciplinary Danish study on ovarian cancer
\cite{Hogdall:2004:Cancer:15160342} where all Danish women diagnosed
with an ovarian tumor and referred for surgery from the participating
departments of gynecology were enrolled continuously from December
1994 to May 1999.
% @Andreas: hvorfor random subsample?
We use a random subsample of $119$ patients with a total of $58$
patients with malignant ovarian cancers as cases and $61$ patients
with benign ovarian tumors as controls. The DACOVA study is another
multidisciplinary Danish study on ovarian cancer which included about
$66\%$ of the female population of Denmark
\cite{bertelsen1991protocol}. It was aimed to continuously enroll all
patients that were referred to surgery of an ovarian tumor clinically
suspected to be cancer during the period 1984 to 1990.
% @Andreas: hvorfor random subsample?
For the purpose of illustration use a random subsample of $113$
patients with a total of $54$ malignant ovarian cancers and $59$
benign ovarian tumors/gynecologic disorders.

Proteomic analysis was performed by MALDI-TOF mass spectrometry with a
Bruker Ultraflex in positive, linear ion mode in the mass-to-charge
range $800$ to $20000$ Dalton by an average of $500$ laser
shots. Calibration of the instrument was performed using a protein
standard mixture. Each spectrum consists of $49642$ samples over the
mass-to-charge range which we down-sample on an equidistant grid of
5000 points by linear interpolation. We further preprocess each
spectrum by first removing the slow-varying baseline intensities with
the SNIP algorithm \cite{ryan1988snip} followed by a normalization
with respect to the total ion counts. Finally, we standardize the 5000
predictors to have column-wise zero mean and unit variance.

We use these two data sets to illustrate how the sequential rank
agreement can be applied in two different scenarios. In the first
scenario we assess the agreement of different statistical
classification methods in how they rank predictors according to
importance for classification accuracy. In the second scenario we
assess the agreement among rankings of the same patients according to
the predicted probabilities of having a malignant tumor. Here we apply
the same statistical classification method to XX bootstrap samples of
the MALOVA data to obtain XX prediction models. From each model we
predict the risks of a malignant patients in the DACOVA study.
Finally we estimate the agreement of the resulting XX rankings of the
DACOVA patients to illustrate the sensitivity of the corresponding
statistical classification method to pertubations of the learning
data. In the latter case we further investigate the behaviour of
different statistical methods in a simulation study.

\subsection{Agreement of predictor ranking}

\begin{figure}[tb]
\begin{center}
\includegraphics[width=.5\textwidth]{pics/marginalVsModel.pdf}
\end{center}
 \caption{Marginal}
 \label{fig:app1}
\end{figure}

\subsection{Agreement of predictions}

\begin{figure}[tb]
\begin{center}
\includegraphics[width=.5\textwidth]{pics/thomas.pdf}
\end{center}
 \caption{Thomas}
 \label{fig:app2}
\end{figure}


\begin{figure}[tb]
\begin{center}
\includegraphics[width=.5\textwidth]{pics/simulationTest2.pdf}
\end{center}
 \caption{SimulationTest2}
 \label{fig:app3}
\end{figure}


\section{Discussion}

Mention/discuss different measures.

Censor lists at significane level?

Bootstrap across a single method and compare results. Discuss collinearity

Changepoint analysis


\bibliographystyle{rss}
\bibliography{paperref}

\end{document}



