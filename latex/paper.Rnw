\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[]{graphicx}
\usepackage{amsmath}
\usepackage[]{color}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{xspace}
\usepackage[noae]{Sweave}

%
% New commands
%
\newcommand{\ie}{\emph{i.e.,}\@\xspace}
\newcommand{\eg}{\emph{e.g,}\@\xspace}


\begin{document}
%\SweaveOpts{concordance=TRUE}

\title{Sequential rank agreement methods for comparison of ranked lists}
\author{}

\maketitle

\begin{abstract}

\end{abstract}

Key words: sequential rank agreement, rank agreement, order statistic,
methods comparison, variable selection

\section{Introduction}

Ranked lists occur in many applications of statistics. Regression
methods rank predictor variables according to magnitude of association
with outcome, prediction models rank subjects according to their risk
of an event, and genetic studies rank genes according to their
difference in expression across samples. A common research question is
where to stop, i.e., to decide the maximal significant rank.  In the
three examples this would correspond to the number of significantly
associated predictor variables, the number of patients at high risk of
an event, and the number of genes that are worth to pursue in further
experiments, respectively.

In this article we describe some new breakthrough tools for measuring
agreement across a set of lists which soon will enter the state-of-the
art. The methods should be useful whenever there are multiple rankings
of the same list. The idea is to define agreement based on the ranks
of the first k elements in each list.


\section{Methods}

Consider a set of $P$ different items $X=\{X_1,\dots,X_P\}$. An
ordered list is a permutation function, $R: \{1,\dots,P\}\to
\{1,\dots,P\}$, such that $R(X_p)$ is the rank of item $X_p$ in the
list. The inverse mapping $\pi=R^{-1}$ assigns to rank
$r\in\{1,\dots,P\}$ the item $\pi(r)$ found at that rank. The methods
described below work for a set of $L$ lists $R_1,\dots,R_L$,
$L\geq2$. We denote $\pi_l=R_l^{-1}$ for the corresponding inverse
mappings. Panels (a) and (b) of Table~\ref{tab:example} show a
schematic example of these mappings.

\begin{table}[tb]
  \caption{Example set of ranked lists. (a) shows the ranked list of items for each of three lists, (b) presents the ranks obtained by each item in each of the three lists and (c) shows the cumulative set of items up to a given depth in the three lists.}
\begin{center}
  \begin{subtable}{4cm}%
    \caption{}
      \begin{tabular}{cccc}
        \hline\hline % & \multicolumn{3}{c}{List} \\
        Rank & $\pi_1$ & $\pi_2$ & $\pi_3$ \\ \hline
        1 & A & A & B \\
        2 & B & C & A \\
        3 & C & D & E \\
        4 & D & B & C \\
        5 & E & E & D \\ \hline
    \end{tabular}
  \end{subtable}
% \hfill
\hspace{1em}
%%% 
  \begin{subtable}{4cm}%
    \caption{}
    \begin{tabular}{cccc} \hline\hline
  % & \multicolumn{3}{c}{List} \\
    Item & $R_1$ & $R_2$ & $R_3$ \\ \hline
    A & 1 & 1 & 2 \\
    B & 2 & 4 & 1 \\
    C & 3 & 2 & 4 \\
    D & 4 & 3 & 5 \\
    E & 5 & 5 & 3 \\ \hline
  \end{tabular}
\end{subtable}
% \hfill
\hspace{1em}
%%%
\begin{subtable}{4cm}%
  \caption{}
\begin{tabular}{cc} \hline\hline
% Depth   &     \\
Depth  &  $S_d$ \\ \hline
1 & $\{$A, B$\}$\\
2 & $\{$A, B, C$\}$ \\
3 & $\{$A, B, C, D, E$\}$ \\
4 & $\{$A, B, C, D, E$\}$ \\
5 & $\{$A, B, C, D, E$\}$ \\ \hline
\end{tabular}
\end{subtable}
\end{center}
\label{tab:example}
\end{table}

The agreement of the lists regarding the rank given to an item can be
measured by
\begin{equation}
  A(p) = f(R_1(p), \ldots, R_L(p)),
\end{equation}
for a distance function $f$. Throughout this paper we will use the
sample standard error as our function $f$ and hence use
$$A(p) = \sqrt{\frac{\sum_{i=1}^L (R_i(p) - \bar{R}(p))^2}{L-1}},
$$
but other choices could be made (see the discussion). The sample
standard error has an interpretation as the average distance of the
individual rankings of the lists from the average ranking.

We now describe what is exemplified in Panel (c) of Table
\ref{tab:example} and how it can be used to define \emph{sequential
  rank agreement}. For an integer $1\le d\le P$ we define the unique
set of items found in the $L$ top $d$ parts of the lists, i.e., the
set of items ranked less than or equal to $d$ in any of the lists:
\begin{equation}
S_d = \{\pi_l(r) ; r \leq d, l = 1, \ldots, L \}.
\end{equation}
The \emph{sequential rank agreement} is the pooled standard deviation
of the items found in the set $S_d$:
\begin{equation}
\textrm{SRA}(d)= \sqrt{\frac{\sum_{\{p \in
      S_d\}}(L-1)A(p)^2}{(L-1)|S_d|}}, \label{def:sra}
\end{equation}
and small values close to zero suggests that the lists agree on the
ordering while larger values suggests disagreement. If the ranked
lists are identical then the value of SRA will be zero for all depths
$d$.  The sequential rank agreement can be interpreted as the average
distance of the individual rankings of the lists from the average
ranking for each of the items we have seen until depth $d$.


\subsection{All lists fully observed}
We shall start by the simplest case where all $L$ lists are fully
observed, \ie, we have the rank of all $P$ items for all of the $L$
lists. This situation occurs is common when we have the original
dataset available and when we wish to, say, compare the results from
different analysis methods.

For the fully obseved list situation we can plot the sequantial
rank agreement \eqref{def:sra} as a function of depth $d$. If there is
a ... An example is seen in Figure~\ref{XX}

\subsection{Analysis of top $k$ lists}
Not uncommon for lists to be 


censored


Let $\Lambda_l, l=1, \ldots, L$ be the set of items found in list $l$
so $\Lambda_l$ is the top $k_l$ list of items from list $l$ where $k_l
= |\Lambda_l|$. Note that we observe the top $k$ items for each of the
$L$ lists if $k_1=\cdots=k_L=k$. For censored lists the rank function
becomes
\begin{equation}
\tilde R_l(p) = \left\{\begin{array}{cl} \{\pi_l^{-1}(p)\} & \text{ for } p\in \Lambda_l \\ 
\{k_l+1,\dots,P\} & \text{ for } p \not\in \Lambda_l\end{array}\right.
\end{equation}
where we only know that the rank for the unobserved items in list $l$ must be larger than the largest rank observed in that list.

In the case of censored lists it is sufficient (FIXME: requires
argument) to look at depths where we have corresponding observations
so the largest rank we should consider will be
\begin{equation}
d \leq \max(k_1, \ldots, k_L).
\end{equation}

We cannot directly compute $A(p)$ for all predictors because we only
observe a censored version of $\tilde R$ for some of the
lists. Instead we assume that the rank assigned to predictor $p$ in
list $l$ is uniformly distributed among the ranks that have \emph{not}
been assigned for list $l$. 

The rankings within a single list are clearly not independent since each of the lists
essentially contains full set of ranks

\begin{equation}
\tilde A(p) = \frac{\sum_{r_1; r_1\in \tilde R_1(p)}  \cdots \sum_{r_L; r_L\in \tilde R_L(p)} A(p)}{\prod_l |\tilde R_l(p)|}
\end{equation}

FIXME: If instead of running through all elements of $\tilde R_1(p)$
one would use the average rank in $\tilde R_1(p)$ we would end up with
a too small variance.


Noget med 

\section{Benchmarks}

Three approaches 

\subsection{Independent lists}

\subsection{Permuted outcomes + analyses}

Do the following a large number of times
\begin{enumerate}
  \item Permute outcome vector
  \item Redo analyses for all $L$ methods
  \item Compute sequential rank agreement for 
\end{enumerate}

Lidt sp√∏js ting man matcher dem op imod

\subsection{Changepoint analysis}

\section{Applications}

\subsection{Comparing results across different method}

In a classical paper by \citet{Golub1999} a dataset of 3051 gene
expression values were measured on 38 tumor mRNA samples in order to
improve the classification of acute leukemias between two types: acute
lymphoblastic leukemia (ALL) or acute myeloid leukemia
(AML). Preprocessing of the gene expression data was done as described
in \citep{Dudoit2002}.

We analyzed these gene expression data using five different
approaches: 

<<>>=
library(SuperRanker)
@ 



\subsection{Stability of results}

\subsection{Evaluating results from top-$k$ lists}



Bootstrap across a single method and compare results. Discuss collinearity


\section{Alternatives}

\section{Discussion}

Mention/discuss different measures. 


\bibliographystyle{apalike}
\bibliography{paperref}

\end{document}



