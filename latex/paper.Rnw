\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[]{graphicx}
\usepackage{amsmath}
\usepackage[]{color}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{xspace}
\usepackage[noae]{Sweave}

%
% New commands
%
\newcommand{\ie}{\emph{i.e.}\@\xspace}
\newcommand{\eg}{\emph{e.g.}\@\xspace}


\begin{document}
%\SweaveOpts{concordance=TRUE}

\title{Sequential rank agreement methods for comparison of ranked lists}
\author{Claus Thorn Ekstrøm and Thomas Alexander Gerds and Kasper Brink-Jensen}

\maketitle

\begin{abstract}
  The comparison of ranked lists has been studied for several decades
  but since the advent of large data types it has received increased
  interest. Computer science and genomics are just two areas that need
  to compare ranked lists in order to in- vestigate similarities
  between different analytical methods. Often the aim is to find a
  measure of similarity for two or more ordered lists that comprise
  the first k items from each list. Since ordered lists typically are
  more similar at the top of the list another approach, which is taken
  in this paper is to ask: for how long are the lists highly similar?
  Using terminology from agreement analysis we derive the measure
  depth of agreement using the standard deviation of the ranks, a
  method that both provides an intuitive interpretation and can be
  applied to any number of lists even if these are incomplete. The
  finding are illustrated using simulations and applied to a publicly
  available cancer dataset.
\end{abstract}

Key words: sequential rank agreement, top lists, rank agreement, order statistic,
methods comparison, variable selection



Multiple lists. Weigh top over bottom. Censoring. Interpretation

\section{Introduction}
Ranking of items or results is common in scientific research and
ranked lists occur as the result of many applications in
statistics. Regression methods rank predictor variables according to
magnitude of association with outcome, prediction models rank subjects
according to their risk of an event, and genetic studies rank genes
according to their difference in gene expression levels across
samples.


When several ranked lists are available a common research question is
to what extent the lists agree on the ranking. In other words: is it
possible to decide on a optimal rank until which the lists agree on
the items?  A typical situation would arise in high-dimensional
genomics studies where several analysis methods had been applied to a
dataset or where a literature search provided differing tables of,
say, top 10 lists of the most important genes associated to an
outcome.  Similarly, this would correspond to the number of associated
predictor variables that are found consistently over the lists, the
number of patients at high risk of an event in all lists, and the
number of genes that are worth to pursue in further experiments in the
three examples above, respectively.

The analysis of rankings is hardly new, and several other approaches
have proposed a measure of the ``distance'' between two ranked
lists. Of these, Kendall's $\tau$ 
%\citep{kendall1948} 
and Spearman’s footrule
%$\rho$ \citep{Spearman1910} 
are the most well-known. Both measure the similarity of two ranked
lists but neither of them make any distinction between errors towards
the top versus the errors towards the bottom of the lists.  Several
modified versions of Kendall's $\tau$ and Spearman's footrule have been
proposed. \citet{Shieh1998} proposed a weighted $\tau$ where each pair
of rankings can be assigned different weights, and \citet{Yilmaz2008}
proposed the $\tau_{ap}$ which places higher emphasis on the top of
the lists.
%
Spearman’s footrule use the ranks of the variables for calculation of
the distance and the use of ranks are also employed in the $M$
measure of \citet{Bar-Ilan2006} where the reciprocal rank differences
are used to calculate the similarity measure.

More recent approaches consider the intersection of lists as the basis
for a similarity measure. However, simple intersections also places
equal weights on all depths of the list which led \citet{Fagin2003}
and \citet{Webber2010} to modify the simple intersection to put more
emphasis on the top of the lists by using the cumulative intersection
divided by the depth as their similarity measures (denoted the average
overlap (AO)). \citet{Webber2010} define their rank-biased overlap
(RBO) by weighting with a converging series thus ensuring the top is
always weighted higher than the (potentially indefinite) bottom of the
lists.  While it is possible to calculate measures that describe how
similar lists are at a given depth with some of the methods described
above, the interpretation of the measures is unclear --- especially
with multiple lists --- and it is difficult to identify a depth at
which the lists are no longer similar.


In this article we propose a method to measure similarity among ranked
lists for each depth in the list. As a measure for similarity we will
use the limits of agreement known from agreement between quantitative
variables \citep{Carstensen2011} but will compute the agreement on the
ranks of the items.  The proposed method allows for multiple lists
simultaneously, provides a dynamic measure of agreement for each depth
in the lists, places more weight on the top of the list, accommodates
censored/incomplete lists of varying lengths, and has a natural
interpretation that directly relates to the ranks. The general idea is
to define agreement based on the sequence of ranks from the first $k$
elements in each list. This approach allows us to estimate a potential
change point in the agreement in order to determine an optimal $k$
where there is a shift in the agreement among lists and enables us to
use randomization approaches to evaluate the rank agreement. In this
sense it is a combination and generalization of some of the ideas of
\citet{Carterette2009} and \citet{Boulesteix2009}. The former compares
two rankings based on the distance between them as measured by a
multivariate Gaussian distribution and the latter presents an overview
of approaches for aggregation of ranked lists including bootstrap and
leave-one-out jackknife approaches.




The manuscript is organized as follows: In the next section we will
define the sequential rank agreement among multiple ranked lists and
discuss how to handle incomplete/censored lists. In section 3 we
present and discuss approaches to evaluate the sequential rank
agreement obtained from an experiment and to identify the optimal
depth $k$ for which the ranked lists agree. Finally we present three
quite different applications of the proposed sequential rank agreement
approach to real data before we discuss the findings along with
possible extensions.



<<echo=FALSE>>=
#note: always pass alpha on the 0-255 scale
makeTransparent<-function(someColor, alpha=100)
{
  newColor<-col2rgb(someColor)
  apply(newColor, 2, function(curcoldata){rgb(red=curcoldata[1], green=curcoldata[2],
    blue=curcoldata[3],alpha=alpha, maxColorValue=255)})
}
@ %

\section{Methods}

Consider a set of $P$ different items $X=\{X_1,\dots,X_P\}$. An
ordered list is a permutation function, $R: \{1,\dots,P\}\to
\{1,\dots,P\}$, such that $R(X_p)$ is the rank of item $X_p$ in the
list. The inverse mapping $\pi=R^{-1}$ assigns to rank
$r\in\{1,\dots,P\}$ the item $\pi(r)$ found at that rank. The methods
described below work for a set of $L$ lists $R_1,\dots,R_L$,
$L\geq2$. We denote $\pi_l=R_l^{-1}$ for the corresponding inverse
mappings. Panels (a) and (b) of Table~\ref{tab:example} show a
schematic example of these mappings. Thus if $\pi_l(1)=X_{34}$ then
item $X_{34}$ is ranked first in list $l$.

\begin{table}[tb]
  \caption{Example set of ranked lists. (a) shows the ranked list of items for each of three lists, (b) presents the ranks obtained by each item in each of the three lists and (c) shows the cumulative set of items up to a given depth in the three lists.}
\begin{center}
  \begin{subtable}{4cm}%
    \caption{}
      \begin{tabular}{cccc}
        \hline\hline % & \multicolumn{3}{c}{List} \\
        Rank & $\pi_1$ & $\pi_2$ & $\pi_3$ \\ \hline
        1 & A & A & B \\
        2 & B & C & A \\
        3 & C & D & E \\
        4 & D & B & C \\
        5 & E & E & D \\ \hline
    \end{tabular}
  \end{subtable}
% \hfill
\hspace{1em}
%%% 
  \begin{subtable}{4cm}%
    \caption{}
    \begin{tabular}{cccc} \hline\hline
  % & \multicolumn{3}{c}{List} \\
    Item & $R_1$ & $R_2$ & $R_3$ \\ \hline
    A & 1 & 1 & 2 \\
    B & 2 & 4 & 1 \\
    C & 3 & 2 & 4 \\
    D & 4 & 3 & 5 \\
    E & 5 & 5 & 3 \\ \hline
  \end{tabular}
\end{subtable}
% \hfill
\hspace{1em}
%%%
\begin{subtable}{4cm}%
  \caption{}
\begin{tabular}{cc} \hline\hline
% Depth   &     \\
Depth  &  $S_d$ \\ \hline
1 & $\{$A, B$\}$\\
2 & $\{$A, B, C$\}$ \\
3 & $\{$A, B, C, D, E$\}$ \\
4 & $\{$A, B, C, D, E$\}$ \\
5 & $\{$A, B, C, D, E$\}$ \\ \hline
\end{tabular}
\end{subtable}
\end{center}
\label{tab:example}
\end{table}

The agreement of the lists regarding the rank given to an item can be
measured by
\begin{equation}
  A(p) = f(R_1(p), \ldots, R_L(p)),
\end{equation}
for a distance function $f$. Throughout this paper we will use the
sample standard error as our function $f$ and hence use
$$A(p) = \sqrt{\frac{\sum_{i=1}^L (R_i(p) - \bar{R}(p))^2}{L-1}},
$$
but other choices could be made (see the discussion). The sample
standard error has an interpretation as the average distance of the
individual rankings of the lists from the average ranking.

We now describe what is exemplified in Panel (c) of Table
\ref{tab:example} and how it can be used to define \emph{sequential
  rank agreement}. For an integer $1\le d\le P$ we define the unique
set of items found in the $L$ top $d$ parts of the lists, i.e., the
set of items ranked less than or equal to $d$ in any of the lists:
\begin{equation}
S_d = \{\pi_l(r) ; r \leq d, l = 1, \ldots, L \}.
\end{equation}
The \emph{sequential rank agreement} is the pooled standard deviation
of the items found in the set $S_d$:
\begin{equation}
\textrm{SRA}(d)= \sqrt{\frac{\sum_{\{p \in
      S_d\}}(L-1)A(p)^2}{(L-1)|S_d|}}, \label{def:sra}
\end{equation}
and small values close to zero suggests that the lists agree on the
ordering while larger values suggests disagreement. If the ranked
lists are identical then the value of SRA will be zero for all depths
$d$.  The sequential rank agreement can be interpreted as the average
distance of the individual rankings of the lists from the average
ranking for each of the items we have seen until depth $d$.


\subsection{Agreement among fully observed lists}
We shall start by the simplest case where all $L$ lists are fully
observed, \ie, we have the rank of all $P$ items for all of the $L$
lists. Fully observed ranked lists are common and occur, for example,
when we have a single dataset and apply different statistical analysis
methods to produce lists of predictors ranked according to their
importance, or if the same analysis method is applied to data from
different populations.

For the fully obseved list situation we can plot the sequantial
rank agreement \eqref{def:sra} as a function of depth $d$. If there is
a ... An example is seen in Figure~\ref{fig:example1}

\subsection{Analysis of incomplete/censored lists}
Not uncommon for lists to be 


censored


Let $\Lambda_l, l=1, \ldots, L$ be the set of items found in list $l$
so $\Lambda_l$ is the top $k_l$ list of items from list $l$ where $k_l
= |\Lambda_l|$. Note that we observe the top $k$ items for each of the
$L$ lists if $k_1=\cdots=k_L=k$. For censored lists the rank function
becomes
\begin{equation}
\tilde R_l(p) = \left\{\begin{array}{cl} \{\pi_l^{-1}(p)\} & \text{ for } p\in \Lambda_l \\ 
\{k_l+1,\dots,P\} & \text{ for } p \not\in \Lambda_l\end{array}\right.
\end{equation}
where we only know that the rank for the unobserved items in list $l$ must be larger than the largest rank observed in that list.

In the case of censored lists it is sufficient (FIXME: requires
argument) to look at depths where we have corresponding observations
so the largest rank we should consider will be
\begin{equation}
d \leq \max(k_1, \ldots, k_L).
\end{equation}

We cannot directly compute $A(p)$ for all predictors because we only
observe a censored version of $\tilde R$ for some of the
lists. Instead we assume that the rank assigned to predictor $p$ in
list $l$ is uniformly distributed among the ranks that have \emph{not}
been assigned for list $l$. 

The rankings within a single list are clearly not independent since each of the lists
essentially contains full set of ranks

\begin{equation}
\tilde A(p) = \frac{\sum_{r_1; r_1\in \tilde R_1(p)}  \cdots \sum_{r_L; r_L\in \tilde R_L(p)} A(p)}{\prod_l |\tilde R_l(p)|}
\end{equation}

FIXME: If instead of running through all elements of $\tilde R_1(p)$
one would use the average rank in $\tilde R_1(p)$ we would end up with
a too small variance.


Noget med 

\section{Benchmarks}

Three approaches 

\subsection{Independent lists}

\subsection{Permuted outcomes + analyses}

Do the following a large number of times
\begin{enumerate}
  \item Permute outcome vector
  \item Redo analyses for all $L$ methods
  \item Compute sequential rank agreement for 
\end{enumerate}

Lidt spøjs ting man matcher dem op imod

\subsection{Changepoint analysis}

\section{Applications}

\subsection{Comparing results across different method}

In a classical paper by \citet{Golub1999} a dataset of 3051 gene
expression values were measured on 38 tumor mRNA samples in order to
improve the classification of acute leukemias between two types: acute
lymphoblastic leukemia (ALL) or acute myeloid leukemia
(AML). Preprocessing of the gene expression data was done as described
in \citep{Dudoit2002}.

We analyzed these gene expression data using four different
approaches: marginal two-sample $t$ tests, marginal logistic
regression analyses, logistic regression eleastic net, and marginal
maximum information content correlations (MIC) \citep{}. For the first
two methods, the genes were ranked according to $p$ value. 


<<example1, echo=FALSE>>=
library(SuperRanker)
library(glmnet)
library(minerva)
library(changepoint)
library(randomForestSRC)

# Read data
library(multtest)
data(golub)


y <- golub.cl
x <- golub 

producelists <- function(x, y) {
    index <- seq(1, nrow(golub))
                                        # d <- data.frame(y, x)

                                        # Marginale t-tests
    mt.p <- sapply(index, function(i) { t.test(x[i,] ~ y)$p.value } )
    list1 <- order(mt.p)
    
                                        # Marginale logreg-tests
    mlogreg.p <- sapply(index, function(i) { drop1(glm(y ~ x[i,], family=binomial), test="Chisq")[2,5] } )
    list2 <- order(mlogreg.p)
    
                                        # Elastic net
    X <- scale(t(x))
    enet <- glmnet(X, y, family="binomial", alpha=.8)
    nyres <- cv.glmnet(X, y, family="binomial", alpha=.8)
    coefficients <- coef(enet, s=nyres$lambda.1se)[-1]
    nonzeros <- sum(coefficients!=0)
    list3 <- order(abs(coefficients), decreasing=TRUE)
    
                                        # MIC
    MIC <- sapply(index, function(i) { mine(x[i,], y)$MIC})
    list4 <- order(MIC, decreasing=TRUE)
    
    
                                        # Random Forest
###    dd <- data.frame(y=factor(y), t(x))
                                        # dd <- data.frame(y=y, t(x))
###    f1 <- rfsrc(y ~ ., data=dd, ntree=100)
###    variables <- abs(f1$importance[,1])
###    num.undecided <- sum(variables==0)
###    list5 <- order(variables, decreasing=TRUE)
###    list5[(length(variables)-num.undecided):length(variables)] <- 0
    
    cbind(list1,list2,list3,list4)
}

inputmatrix <- producelists(x, y)
colnames(inputmatrix) <- c("T", "LogReg", "ElasticNet", "MIC", "RF")[1:ncol(inputmatrix)]

res <- sqrt(sra(inputmatrix))

@ 


\begin{figure}[tb]
\begin{center}
<<fig=TRUE,echo=FALSE>>=
ysize <- 1000
plot(res[1:ysize], lwd=3, col="red", type="l", ylim=c(0,1300), ylab="Sequential rank agreement", xlab="Depth")

## 
# col1 <- makeTransparent()
null <- sqrt(random_list_sra(inputmatrix, B=1, n=400))
## matlines(null[1:ysize,], col="gray", lty=1)

## 

##null2 <- sapply(1:40, function(i) {
##    sra(producelists(x, sample(y)), B=1)
##} )
## matlines(null2[1:ysize,], col="lightblue", lty=1)

load("R/fig1null.rda")

bcolor <- makeTransparent("lightblue", alpha=80)
bcolor2 <- makeTransparent("red", alpha=80)

www <- smooth_sra(null2)
polygon(c(1:length(www$lower), rev(1:length(www$lower))),
        c(www$lower, rev(www$upper)),
        col=bcolor, border=NA)

www <- smooth_sra(null)
polygon(c(1:length(www$lower), rev(1:length(www$lower))),
        c(www$lower, rev(www$upper)),
        col=bcolor2, border=NA)



@ %
\end{center}
 \label{fig:example1}
 \caption{Sequential rank agreement for 4 different analysis methods
   applied to the Golub data (thick red line). The red and blue shaded
 areas represent the pointwise 95\% normal areas for the XXX and XXY benchmarks,
respectively.}
\end{figure}


<<echo=FALSE, results=hide>>=
xxx <- as.data.frame(inputmatrix[1:10,])
xxx

@ 

<<label=tab1,echo=FALSE,results=tex>>=
library(xtable)
print(xtable(xxx))
#print(xtable(xxx, caption = "Top 10 list of results", label = "tab:one",
#digits = c(0, 0, 2, 0, 2, 3, 3)), table.placement = "tbp",
#caption.placement = "top")

@


\subsection{Stability of results}

\subsection{Evaluating results from top-$k$ lists}



Bootstrap across a single method and compare results. Discuss collinearity


\section{Alternatives}

\section{Discussion}

Mention/discuss different measures. 


\bibliographystyle{apalike}
\bibliography{paperref}

\end{document}



